{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datasetName</th>\n",
       "      <th>about</th>\n",
       "      <th>link</th>\n",
       "      <th>categoryName</th>\n",
       "      <th>cloud</th>\n",
       "      <th>vintage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Microbiome Project</td>\n",
       "      <td>American Gut (Microbiome Project)</td>\n",
       "      <td>https://github.com/biocore/American-Gut</td>\n",
       "      <td>Biology</td>\n",
       "      <td>GitHub</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GloBI</td>\n",
       "      <td>Global Biotic Interactions (GloBI)</td>\n",
       "      <td>https://github.com/jhpoelen/eol-globi-data/wik...</td>\n",
       "      <td>Biology</td>\n",
       "      <td>GitHub</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Global Climate</td>\n",
       "      <td>Global Climate Data Since 1929</td>\n",
       "      <td>http://en.tutiempo.net/climate</td>\n",
       "      <td>Climate/Weather</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1929.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CommonCraw 2012</td>\n",
       "      <td>3.5B Web Pages from CommonCraw 2012</td>\n",
       "      <td>http://www.bigdatanews.com/profiles/blogs/big-...</td>\n",
       "      <td>Computer Networks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indiana Webclicks</td>\n",
       "      <td>53.5B Web clicks of 100K users in Indiana Univ.</td>\n",
       "      <td>http://cnets.indiana.edu/groups/nan/webtraffic...</td>\n",
       "      <td>Computer Networks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          datasetName                                            about  \\\n",
       "0  Microbiome Project                American Gut (Microbiome Project)   \n",
       "1               GloBI               Global Biotic Interactions (GloBI)   \n",
       "2      Global Climate                   Global Climate Data Since 1929   \n",
       "3     CommonCraw 2012              3.5B Web Pages from CommonCraw 2012   \n",
       "4   Indiana Webclicks  53.5B Web clicks of 100K users in Indiana Univ.   \n",
       "\n",
       "                                                link       categoryName  \\\n",
       "0            https://github.com/biocore/American-Gut            Biology   \n",
       "1  https://github.com/jhpoelen/eol-globi-data/wik...            Biology   \n",
       "2                     http://en.tutiempo.net/climate    Climate/Weather   \n",
       "3  http://www.bigdatanews.com/profiles/blogs/big-...  Computer Networks   \n",
       "4  http://cnets.indiana.edu/groups/nan/webtraffic...  Computer Networks   \n",
       "\n",
       "    cloud  vintage  \n",
       "0  GitHub      NaN  \n",
       "1  GitHub      NaN  \n",
       "2     NaN   1929.0  \n",
       "3     NaN   2012.0  \n",
       "4     NaN      NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\adity\\Downloads\\datasets.csv\")\n",
    "df.head()  # Display the first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Local\\Temp\\ipykernel_42228\\826076795.py:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df.fillna(\"\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df.fillna(\"\", inplace=True)\n",
    "df[\"combined_text\"] = df[\"datasetName\"] + \" \" + df[\"about\"] + \" \" + df[\"categoryName\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df[\"category_encoded\"] = label_encoder.fit_transform(df[\"categoryName\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "df[[\"category_encoded\"]] = scaler.fit_transform(df[[\"category_encoded\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode dataset names to numerical labels\n",
    "dataset_encoder = LabelEncoder()\n",
    "df[\"dataset_encoded\"] = dataset_encoder.fit_transform(df[\"datasetName\"])\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[[\"category_encoded\"]], df[\"dataset_encoded\"], test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(input_dim=len(df[\"categoryName\"].unique()) + 1, output_dim=16),\n",
    "    LSTM(32, return_sequences=True),\n",
    "    LSTM(16),\n",
    "    Dense(8, activation='relu'),\n",
    "    Dense(len(df[\"datasetName\"].unique()), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 459ms/step - accuracy: 0.0000e+00 - loss: 4.1112 - val_accuracy: 0.0000e+00 - val_loss: 4.1134\n",
      "Epoch 2/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.0339 - loss: 4.1101 - val_accuracy: 0.0000e+00 - val_loss: 4.1163\n",
      "Epoch 3/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.0182 - loss: 4.1092 - val_accuracy: 0.0000e+00 - val_loss: 4.1193\n",
      "Epoch 4/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.0182 - loss: 4.1083 - val_accuracy: 0.0000e+00 - val_loss: 4.1223\n",
      "Epoch 5/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.0182 - loss: 4.1076 - val_accuracy: 0.0000e+00 - val_loss: 4.1253\n",
      "Epoch 6/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0339 - loss: 4.1065 - val_accuracy: 0.0000e+00 - val_loss: 4.1284\n",
      "Epoch 7/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.0339 - loss: 4.1055 - val_accuracy: 0.0000e+00 - val_loss: 4.1316\n",
      "Epoch 8/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.0104 - loss: 4.1053 - val_accuracy: 0.0000e+00 - val_loss: 4.1347\n",
      "Epoch 9/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.0104 - loss: 4.1041 - val_accuracy: 0.0000e+00 - val_loss: 4.1380\n",
      "Epoch 10/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.0104 - loss: 4.1033 - val_accuracy: 0.0000e+00 - val_loss: 4.1412\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1eb062a2ba0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
      "        datasetName                                         about  \\\n",
      "56   ClueWeb12 FACC                                ClueWeb12 FACC   \n",
      "35        Formula 1  Ergast Formula 1, from 1950 up to date (API)   \n",
      "37      Airlines OD                    Airlines OD Data 1987-2008   \n",
      "13  World countries           World countries in multiple formats   \n",
      "8   Localytics Data       Localytics Data Visualization Challenge   \n",
      "\n",
      "                                                 link      categoryName  \n",
      "56           http://lemurproject.org/clueweb12/FACC1/  Natural Language  \n",
      "35                           http://ergast.com/mrd/db            Sports  \n",
      "37  http://stat-computing.org/dataexpo/2009/the-da...    Transportation  \n",
      "13               https://github.com/mledoze/countries               GIS  \n",
      "8    https://github.com/localytics/data-viz-challenge   Data Challenges  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adity\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from difflib import get_close_matches\n",
    "\n",
    "def find_closest_category(user_input, categories):\n",
    "    matches = get_close_matches(user_input, categories, n=1, cutoff=0.4)\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "def recommend_datasets(user_input):\n",
    "    # Find closest matching category\n",
    "    closest_category = find_closest_category(user_input, label_encoder.classes_)\n",
    "    \n",
    "    if closest_category is None:\n",
    "        return \"No related datasets found.\"\n",
    "\n",
    "    user_vec = label_encoder.transform([closest_category])  # Use closest match\n",
    "    user_vec = scaler.transform([user_vec])\n",
    "    \n",
    "    predictions = model.predict(user_vec)\n",
    "    top_indices = predictions.argsort()[0][-5:][::-1]\n",
    "\n",
    "    recommended_datasets = df.iloc[top_indices][[\"datasetName\", \"about\", \"link\", \"categoryName\"]]\n",
    "\n",
    "    if recommended_datasets.empty:\n",
    "        return \"No related datasets found.\"\n",
    "    \n",
    "    return recommended_datasets\n",
    "\n",
    "# Example usage\n",
    "user_domain = input(\"Enter your domain: \")\n",
    "recommendations = recommend_datasets(user_domain)\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
