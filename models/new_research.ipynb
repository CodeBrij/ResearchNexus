{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
       "      <td>Stereo matching is one of the widely used tech...</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>The recent advancements in artificial intellig...</td>\n",
       "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
       "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
       "      <td>['cs.CV', 'cs.AI']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
       "      <td>Consistency training has proven to be an advan...</td>\n",
       "      <td>['cs.CV']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Background-Foreground Segmentation for Interio...</td>\n",
       "      <td>To ensure safety in automated driving, the cor...</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titles  \\\n",
       "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
       "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
       "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
       "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
       "4  Background-Foreground Segmentation for Interio...   \n",
       "\n",
       "                                           summaries  \\\n",
       "0  Stereo matching is one of the widely used tech...   \n",
       "1  The recent advancements in artificial intellig...   \n",
       "2  In this paper, we proposed a novel mutual cons...   \n",
       "3  Consistency training has proven to be an advan...   \n",
       "4  To ensure safety in automated driving, the cor...   \n",
       "\n",
       "                         terms  \n",
       "0           ['cs.CV', 'cs.LG']  \n",
       "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
       "2           ['cs.CV', 'cs.AI']  \n",
       "3                    ['cs.CV']  \n",
       "4           ['cs.CV', 'cs.LG']  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\adity\\Downloads\\arxiv_data.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources downloaded successfully!\n",
      "Dataset loaded with 51774 papers\n",
      "Preprocessing data...\n",
      "Creating TF-IDF vectors...\n",
      "TF-IDF matrix shape: (51774, 5000)\n",
      "\n",
      "Testing recommendation system...\n",
      "\n",
      "Recommendations for query: 'deep learning for computer vision'\n",
      "\n",
      "Recommendation 1: (Similarity: 0.5520)\n",
      "Title: Are object detection assessment criteria ready for maritime computer vision?\n",
      "Summary: Maritime vessels equipped with visible and infrared cameras can complement\n",
      "other conventional sensors for object detection. However, application of\n",
      "computer vision techniques in maritime domain receiv...\n",
      "Terms: ['cs.CV']\n",
      "--------------------------------------------------------------------------------\n",
      "Recommendation 2: (Similarity: 0.5520)\n",
      "Title: Are object detection assessment criteria ready for maritime computer vision?\n",
      "Summary: Maritime vessels equipped with visible and infrared cameras can complement\n",
      "other conventional sensors for object detection. However, application of\n",
      "computer vision techniques in maritime domain receiv...\n",
      "Terms: ['cs.CV']\n",
      "--------------------------------------------------------------------------------\n",
      "Recommendation 3: (Similarity: 0.4849)\n",
      "Title: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\n",
      "Summary: Deep reinforcement learning augments the reinforcement learning framework and\n",
      "utilizes the powerful representation of deep neural networks. Recent works have\n",
      "demonstrated the remarkable successes of d...\n",
      "Terms: ['cs.CV', 'cs.AI']\n",
      "--------------------------------------------------------------------------------\n",
      "Recommendation 4: (Similarity: 0.4849)\n",
      "Title: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\n",
      "Summary: Deep reinforcement learning augments the reinforcement learning framework and\n",
      "utilizes the powerful representation of deep neural networks. Recent works have\n",
      "demonstrated the remarkable successes of d...\n",
      "Terms: ['cs.CV', 'cs.AI']\n",
      "--------------------------------------------------------------------------------\n",
      "Recommendation 5: (Similarity: 0.4849)\n",
      "Title: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\n",
      "Summary: Deep reinforcement learning augments the reinforcement learning framework and\n",
      "utilizes the powerful representation of deep neural networks. Recent works have\n",
      "demonstrated the remarkable successes of d...\n",
      "Terms: ['cs.CV', 'cs.AI']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "To use interactive mode, run: interactive_recommendation()\n"
     ]
    }
   ],
   "source": [
    "# Research Paper Recommendation System\n",
    "# This system recommends relevant research papers based on user queries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "# First, install and download NLTK resources\n",
    "# Run these commands once to download the necessary resources\n",
    "import nltk\n",
    "\n",
    "# Download NLTK resources explicitly - this is important!\n",
    "try:\n",
    "    # Try to download resources\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    print(\"NLTK resources downloaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK resources: {e}\")\n",
    "    print(\"Proceeding with a simplified preprocessing function...\")\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\adity\\Downloads\\arxiv_data.csv\")\n",
    "print(f\"Dataset loaded with {len(df)} papers\")\n",
    "\n",
    "# Define a simpler preprocessing function that doesn't rely on NLTK resources\n",
    "def simple_preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        return text\n",
    "    return ''\n",
    "\n",
    "# Apply preprocessing to titles and summaries\n",
    "print(\"Preprocessing data...\")\n",
    "df['processed_title'] = df['titles'].apply(simple_preprocess_text)\n",
    "df['processed_summary'] = df['summaries'].apply(simple_preprocess_text)\n",
    "\n",
    "# Combine title and summary for better feature representation\n",
    "df['combined_features'] = df['processed_title'] + ' ' + df['processed_summary']\n",
    "\n",
    "# Extract terms as text (they appear to be in list format)\n",
    "def extract_terms(term_str):\n",
    "    try:\n",
    "        # Try to evaluate the string as a list\n",
    "        if isinstance(term_str, str):\n",
    "            if term_str.startswith('[') and term_str.endswith(']'):\n",
    "                return ' '.join(eval(term_str))\n",
    "            return term_str\n",
    "        return ''\n",
    "    except:\n",
    "        # If evaluation fails, return the original string\n",
    "        return term_str if isinstance(term_str, str) else ''\n",
    "\n",
    "df['terms_text'] = df['terms'].apply(extract_terms)\n",
    "\n",
    "# Add terms to the features\n",
    "df['features'] = df['combined_features'] + ' ' + df['terms_text']\n",
    "\n",
    "# Create TF-IDF vectors\n",
    "print(\"Creating TF-IDF vectors...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['features'])\n",
    "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "\n",
    "# Function to get paper recommendations\n",
    "def get_paper_recommendations(user_query, top_n=5):\n",
    "    # Preprocess the user query\n",
    "    processed_query = simple_preprocess_text(user_query)\n",
    "    \n",
    "    # Transform the query to TF-IDF vector\n",
    "    query_vector = tfidf_vectorizer.transform([processed_query])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    \n",
    "    # Get indices of top N most similar papers\n",
    "    top_indices = similarity_scores.argsort()[:-top_n-1:-1]\n",
    "    \n",
    "    # Return the recommendations\n",
    "    recommendations = df.iloc[top_indices][['titles', 'summaries', 'terms']]\n",
    "    \n",
    "    return recommendations, similarity_scores[top_indices]\n",
    "\n",
    "# Example usage\n",
    "def recommend_papers(user_query, top_n=5):\n",
    "    recommendations, scores = get_paper_recommendations(user_query, top_n)\n",
    "    \n",
    "    print(f\"\\nRecommendations for query: '{user_query}'\\n\")\n",
    "    \n",
    "    for i, (_, row) in enumerate(recommendations.iterrows()):\n",
    "        print(f\"Recommendation {i+1}: (Similarity: {scores[i]:.4f})\")\n",
    "        print(f\"Title: {row['titles']}\")\n",
    "        print(f\"Summary: {row['summaries'][:200]}...\")\n",
    "        print(f\"Terms: {row['terms']}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Test the recommendation system\n",
    "print(\"\\nTesting recommendation system...\")\n",
    "user_query = \"deep learning for computer vision\"\n",
    "recommend_papers(user_query)\n",
    "\n",
    "# Interactive recommendation function\n",
    "def interactive_recommendation():\n",
    "    while True:\n",
    "        query = input(\"\\nEnter your research interest (or 'exit' to quit): \")\n",
    "        if query.lower() == 'exit':\n",
    "            break\n",
    "        n = input(\"How many recommendations do you want? (default: 5): \")\n",
    "        try:\n",
    "            n = int(n)\n",
    "        except:\n",
    "            n = 5\n",
    "        recommend_papers(query, n)\n",
    "\n",
    "# Run the interactive recommendation system\n",
    "print(\"\\nTo use interactive mode, run: interactive_recommendation()\")\n",
    "# interactive_recommendation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing recommendation system with a sample query...\n",
      "\n",
      "Recommendations for query: 'deep learning for computer vision'\n",
      "\n",
      "Recommendation 1: (Similarity: 0.5520)\n",
      "Title: Are object detection assessment criteria ready for maritime computer vision?\n",
      "Summary: Maritime vessels equipped with visible and infrared cameras can complement\n",
      "other conventional sensors for object detection. However, application of\n",
      "computer vision techniques in maritime domain receiv...\n",
      "Terms: ['cs.CV']\n",
      "--------------------------------------------------------------------------------\n",
      "Recommendation 2: (Similarity: 0.5520)\n",
      "Title: Are object detection assessment criteria ready for maritime computer vision?\n",
      "Summary: Maritime vessels equipped with visible and infrared cameras can complement\n",
      "other conventional sensors for object detection. However, application of\n",
      "computer vision techniques in maritime domain receiv...\n",
      "Terms: ['cs.CV']\n",
      "--------------------------------------------------------------------------------\n",
      "Recommendation 3: (Similarity: 0.4849)\n",
      "Title: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\n",
      "Summary: Deep reinforcement learning augments the reinforcement learning framework and\n",
      "utilizes the powerful representation of deep neural networks. Recent works have\n",
      "demonstrated the remarkable successes of d...\n",
      "Terms: ['cs.CV', 'cs.AI']\n",
      "--------------------------------------------------------------------------------\n",
      "Recommendation 4: (Similarity: 0.4849)\n",
      "Title: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\n",
      "Summary: Deep reinforcement learning augments the reinforcement learning framework and\n",
      "utilizes the powerful representation of deep neural networks. Recent works have\n",
      "demonstrated the remarkable successes of d...\n",
      "Terms: ['cs.CV', 'cs.AI']\n",
      "--------------------------------------------------------------------------------\n",
      "Recommendation 5: (Similarity: 0.4849)\n",
      "Title: Deep Reinforcement Learning in Computer Vision: A Comprehensive Survey\n",
      "Summary: Deep reinforcement learning augments the reinforcement learning framework and\n",
      "utilizes the powerful representation of deep neural networks. Recent works have\n",
      "demonstrated the remarkable successes of d...\n",
      "Terms: ['cs.CV', 'cs.AI']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "To use interactive mode, run: interactive_recommendation()\n"
     ]
    }
   ],
   "source": [
    "# Evaluate with synthetic queries - simplified version\n",
    "def evaluate_model_with_synthetic_queries():\n",
    "    # Create synthetic queries from paper titles\n",
    "    num_samples = min(50, len(df))\n",
    "    test_indices = np.random.choice(len(df), num_samples, replace=False)\n",
    "    test_df = df.iloc[test_indices]\n",
    "    \n",
    "    # Get keywords from titles to use as queries\n",
    "    def extract_keywords(title, n=3):\n",
    "        words = title.split()\n",
    "        if len(words) <= n:\n",
    "            return title\n",
    "        else:\n",
    "            return ' '.join(np.random.choice(words, n, replace=False))\n",
    "    \n",
    "    test_queries = test_df['titles'].apply(extract_keywords).tolist()\n",
    "    \n",
    "    # Store results\n",
    "    hit_rates = []  # Did the original paper appear in top 10?\n",
    "    avg_positions = []  # Average position of the original paper\n",
    "    avg_similarities = []  # Average similarity score\n",
    "    \n",
    "    print(\"Evaluating model with synthetic queries...\")\n",
    "    for i, query in enumerate(test_queries):\n",
    "        original_idx = test_indices[i]\n",
    "        \n",
    "        # Get recommendations\n",
    "        _, similarity_scores, top_indices = get_paper_recommendations(query, top_n=10)\n",
    "        avg_similarities.append(np.mean(similarity_scores))\n",
    "        \n",
    "        # Check if original paper is in recommendations\n",
    "        if original_idx in top_indices:\n",
    "            hit_rates.append(1)\n",
    "            position = np.where(top_indices == original_idx)[0][0] + 1  # Position (1-based)\n",
    "            avg_positions.append(position)\n",
    "        else:\n",
    "            hit_rates.append(0)\n",
    "            avg_positions.append(11)  # Not found, assign position beyond top 10\n",
    "    \n",
    "    # Calculate metrics\n",
    "    hit_rate = np.mean(hit_rates)\n",
    "    mean_position = np.mean([p for p in avg_positions if p <= 10])  # Average position when found\n",
    "    mean_similarity = np.mean(avg_similarities)\n",
    "    \n",
    "    return {\n",
    "        'hit_rate': hit_rate,\n",
    "        'mean_position': mean_position,\n",
    "        'mean_similarity': mean_similarity,\n",
    "        'all_positions': avg_positions,\n",
    "        'all_similarities': avg_similarities\n",
    "    }\n",
    "\n",
    "# Visualize similarity distribution\n",
    "def plot_similarity_distribution(similarities):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(similarities, kde=True)\n",
    "    plt.title('Distribution of Similarity Scores')\n",
    "    plt.xlabel('Similarity Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.savefig('similarity_distribution.png')\n",
    "    plt.close()\n",
    "    return 'similarity_distribution.png'\n",
    "\n",
    "# Visualize performance metrics\n",
    "def plot_performance_metrics(eval_results):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    metrics = ['Hit Rate', 'Mean Position\\n(lower is better)']\n",
    "    values = [eval_results['hit_rate'], min(10, eval_results['mean_position'])/10]  # Normalize position to 0-1 scale\n",
    "    \n",
    "    colors = ['#5DA5DA', '#FAA43A']\n",
    "    plt.bar(metrics, values, color=colors)\n",
    "    plt.title('Model Performance Metrics')\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add actual values as text\n",
    "    plt.text(0, values[0] + 0.05, f\"{eval_results['hit_rate']:.2f}\", ha='center')\n",
    "    plt.text(1, values[1] + 0.05, f\"{eval_results['mean_position']:.1f}\", ha='center')\n",
    "    \n",
    "    plt.savefig('performance_metrics.png')\n",
    "    plt.close()\n",
    "    return 'performance_metrics.png'\n",
    "\n",
    "# Plot rank distribution\n",
    "def plot_rank_distribution(positions):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    positions_in_top10 = [p for p in positions if p <= 10]\n",
    "    if positions_in_top10:\n",
    "        sns.countplot(x=positions_in_top10)\n",
    "        plt.title('Distribution of Recommendation Ranks')\n",
    "        plt.xlabel('Rank Position')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.savefig('rank_distribution.png')\n",
    "        plt.close()\n",
    "        return 'rank_distribution.png'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Extract and visualize term distribution\n",
    "def analyze_term_distribution():\n",
    "    # Generate some test queries\n",
    "    test_queries = [\n",
    "        \"deep learning\", \n",
    "        \"computer vision\", \n",
    "        \"natural language processing\", \n",
    "        \"reinforcement learning\",\n",
    "        \"graph neural networks\"\n",
    "    ]\n",
    "    \n",
    "    all_recommended_terms = []\n",
    "    query_term_dict = {}\n",
    "    \n",
    "    for query in test_queries:\n",
    "        query_terms = []\n",
    "        recommendations, _, _ = get_paper_recommendations(query, top_n=10)\n",
    "        for _, row in recommendations.iterrows():\n",
    "            try:\n",
    "                term_str = row['terms']\n",
    "                if isinstance(term_str, str) and term_str.startswith('[') and term_str.endswith(']'):\n",
    "                    terms = eval(term_str)\n",
    "                    all_recommended_terms.extend(terms)\n",
    "                    query_terms.extend(terms)\n",
    "            except:\n",
    "                continue\n",
    "        query_term_dict[query] = Counter(query_terms).most_common(5)\n",
    "    \n",
    "    # Count term frequencies\n",
    "    term_counts = Counter(all_recommended_terms).most_common(15)\n",
    "    \n",
    "    # Visualize overall distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    terms, counts = zip(*term_counts)\n",
    "    plt.barh(terms, counts, color=sns.color_palette(\"viridis\", len(terms)))\n",
    "    plt.title('Most Common Terms in Recommendations')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('term_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Visualize per-query distribution\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    for i, (query, query_terms) in enumerate(query_term_dict.items()):\n",
    "        plt.subplot(len(query_term_dict), 1, i+1)\n",
    "        if query_terms:\n",
    "            terms, counts = zip(*query_terms)\n",
    "            plt.barh(terms, counts, color=sns.color_palette(\"Set2\", len(terms)))\n",
    "            plt.title(f'Top Terms for Query: \"{query}\"')\n",
    "            plt.xlabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('query_term_distribution.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return 'term_distribution.png', 'query_term_distribution.png', term_counts\n",
    "\n",
    "\n",
    "# Test the recommendation system\n",
    "print(\"\\nTesting recommendation system with a sample query...\")\n",
    "user_query = \"deep learning for computer vision\"\n",
    "recommend_papers(user_query)\n",
    "\n",
    "print(\"\\nTo use interactive mode, run: interactive_recommendation()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
